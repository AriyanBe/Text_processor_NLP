Noise removal — stripping text of formatting.

Tokenization — breaking text into individual words.

Normalization — cleaning text data in any other way:

Stemming is a blunt axe to chop off word prefixes and suffixes.

Lemmatization is a scalpel to bring words down to their root forms.
